# -*- coding: utf-8 -*-
"""DataV project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e-iGXtX8bY_lgEEZSZ82O-slpXRiH9p1
"""

# Commented out IPython magic to ensure Python compatibility.
# Global imports & settings
import numpy as np
import pandas as pd

# Visualization (EDA in Colab)
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

# Statistics & utilities
from scipy import stats

# Date and time helpers
from datetime import datetime

# Install Dash and Dash Bootstrap Components if not already installed
!pip install dash dash-bootstrap-components

# Dash (for the interactive website)
from dash import Dash, dcc, html, Input, Output, State, callback_context
import dash_bootstrap_components as dbc

# System / path
import os
from pathlib import Path

# Display options for pandas
pd.set_option("display.max_columns", 100)
pd.set_option("display.width", 120)

# Matplotlib inline for Colab
# %matplotlib inline

"""# **# Phase 1 – Data Loading and Raw Overview**
## 1.1 Load crashes and persons datasets
- `df_persons`: person-level records linked by `COLLISION_ID`

"""

df_crashes = pd.read_csv("/content/Motor_Vehicle_Collisions_-_Crashes.csv", engine='python', on_bad_lines='warn')
df_persons = pd.read_csv("/content/Motor_Vehicle_Collisions_-_Person.csv")

display(df_crashes.head())
display(df_persons.head())

"""## 1.2 Basic size and columns overview

Here we check:
- Number of rows and columns for each dataset
- Full column list (to understand what is available)
"""

print("Crashes dataset shape (rows, columns):", df_crashes.shape)
print("Persons dataset shape (rows, columns):", df_persons.shape)

print("\nCrashes columns:")
print(list(df_crashes.columns))

print("\nPersons columns:")
print(list(df_persons.columns))

"""## 1.3 Info, data types, and memory usage

We inspect:
- Data types of each column
- Non-null counts (very important for missing values later)
- Rough memory usage

"""

print("=== df_crashes.info() ===")
df_crashes.info(memory_usage="deep")

print("\n=== df_persons.info() ===")
df_persons.info(memory_usage="deep")

"""## 1.4 Quick numerical summary
Get basic statistics of numerical columns to spot:
- Ranges
- Possible impossible values
- General scale of injuries/fatalities
"""

crashes_desc = df_crashes.describe().T
persons_desc = df_persons.describe().T

display(crashes_desc.head(20))   # limit to first 20 rows to keep it readable
display(persons_desc.head(20))

"""## 1.5 Save raw copies (optional)

Save the raw versions of both datasets so we always have an untouched copy to refer back to.

"""

df_crashes_raw = df_crashes.copy()
df_persons_raw = df_persons.copy()

"""# **Phase 2 – Initial EDA (Pre-cleaning)**

In this phase we:
- Explore key categorical and numerical distributions
- Look at basic time patterns (year, month, hour, weekday)
- Do a quick spatial look (boroughs + rough map)
- Check missing values and obvious data quality issues

"""

cat_cols = [
    "BOROUGH",
    "CONTRIBUTING FACTOR VEHICLE 1",
    "VEHICLE TYPE CODE 1"
]

num_cols = [
    "NUMBER OF PERSONS INJURED",
    "NUMBER OF PERSONS KILLED",
    "NUMBER OF PEDESTRIANS INJURED",
    "NUMBER OF CYCLIST INJURED",
    "NUMBER OF MOTORIST INJURED"
]

# Categorical: value counts (top categories)
for c in cat_cols:
    if c in df_crashes.columns:
        print(f"\n=== {c} – value counts (top 10) ===")
        display(df_crashes[c].value_counts(dropna=False).head(10))

# Numeric: summary + simple histograms
for c in num_cols:
    if c in df_crashes.columns:
        print(f"\n=== {c} – describe() ===")
        display(df_crashes[c].describe())

        plt.figure()
        df_crashes[c].hist(bins=20)
        plt.title(f"Histogram of {c}")
        plt.xlabel(c)
        plt.ylabel("Count")
        plt.show()

"""# Phase 2.2 – Temporal features

"""

# Make a datetime column (errors='coerce' turns bad rows into NaT)
if "CRASH DATE" in df_crashes.columns and "CRASH TIME" in df_crashes.columns:
    df_crashes["CRASH_DATETIME"] = pd.to_datetime(
        df_crashes["CRASH DATE"].astype(str) + " " + df_crashes["CRASH TIME"].astype(str),
        errors="coerce"
    )

    # Extract parts
    df_crashes["CRASH_YEAR"] = df_crashes["CRASH_DATETIME"].dt.year
    df_crashes["CRASH_MONTH"] = df_crashes["CRASH_DATETIME"].dt.to_period("M").astype(str)
    df_crashes["CRASH_HOUR"] = df_crashes["CRASH_DATETIME"].dt.hour
    df_crashes["CRASH_WEEKDAY"] = df_crashes["CRASH_DATETIME"].dt.day_name()

    # Crashes per year
    crashes_per_year = df_crashes["CRASH_YEAR"].value_counts().sort_index()
    print("Crashes per year:")
    display(crashes_per_year)

    # Line plot: crashes by year
    plt.figure()
    crashes_per_year.plot(kind="line", marker="o")
    plt.title("Number of crashes per year")
    plt.xlabel("Year")
    plt.ylabel("Number of crashes")
    plt.show()

    # Crashes per month-period (YYYY-MM)
    crashes_per_month = df_crashes["CRASH_MONTH"].value_counts().sort_index()
    print("\nCrashes per month (period):")
    display(crashes_per_month.head(24))  # show first 24 for readability

    # Crashes by hour of day
    crashes_by_hour = df_crashes["CRASH_HOUR"].value_counts().sort_index()
    plt.figure()
    crashes_by_hour.plot(kind="bar")
    plt.title("Crashes by hour of day")
    plt.xlabel("Hour (0–23)")
    plt.ylabel("Number of crashes")
    plt.show()

    # Crashes by weekday (order manually)
    weekday_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
    crashes_by_weekday = df_crashes["CRASH_WEEKDAY"].value_counts()
    crashes_by_weekday = crashes_by_weekday.reindex(weekday_order)

    plt.figure()
    crashes_by_weekday.plot(kind="bar")
    plt.title("Crashes by weekday")
    plt.xlabel("Weekday")
    plt.ylabel("Number of crashes")
    plt.show()
else:
    print("CRASH DATE / CRASH TIME columns not found – please check column names.")

"""# Phase 2.3 – Spatial patterns (borough + rough map)

"""

if "BOROUGH" in df_crashes.columns:
    crashes_by_borough = df_crashes["BOROUGH"].value_counts()
    print("Crashes by borough:")
    display(crashes_by_borough)

    plt.figure()
    crashes_by_borough.plot(kind="bar")
    plt.title("Number of crashes by borough")
    plt.xlabel("Borough")
    plt.ylabel("Number of crashes")
    plt.show()

# 2) Rough scatter on latitude/longitude (sample for speed)
lat_col = "LATITUDE"
lon_col = "LONGITUDE"

if lat_col in df_crashes.columns and lon_col in df_crashes.columns:
    # Drop rows with missing coords
    df_geo = df_crashes[[lat_col, lon_col, "BOROUGH"]].dropna()

    # Sample to avoid a massive plot
    df_geo_sample = df_geo.sample(n=min(5000, len(df_geo)), random_state=42)

    fig = px.scatter(
        df_geo_sample,
        x=lon_col,
        y=lat_col,
        color="BOROUGH",
        title="Sample of crash locations (scatter, not real map projection)",
        labels={lon_col: "Longitude", lat_col: "Latitude"}
    )
    fig.show()
else:
    print("Latitude/Longitude columns not found – please check column names.")

"""# Phase 2.4 – Missing values and simple quality checks

"""

# Missing values % per column (crashes)
missing_crashes = (df_crashes.isna().sum() / len(df_crashes) * 100).sort_values(ascending=False)
print("Missing values in df_crashes (percent):")
display(missing_crashes.head(25))

# Missing values % per column (persons)
missing_persons = (df_persons.isna().sum() / len(df_persons) * 100).sort_values(ascending=False)
print("\nMissing values in df_persons (percent):")
display(missing_persons.head(25))

# Simple impossible values check for some numeric columns
check_cols = [
    "NUMBER OF PERSONS INJURED",
    "NUMBER OF PERSONS KILLED",
    "NUMBER OF PEDESTRIANS INJURED",
    "NUMBER OF PEDESTRIANS KILLED"
]

for c in check_cols:
    if c in df_crashes.columns:
        num_negative = (df_crashes[c] < 0).sum()
        print(f"{c}: negative values count = {num_negative}")

"""# **Phase 3 – Pre-Integration Cleaning**

This phase focuses on:

Missing values

Dtype fixes (especially datetime)

Outliers

Duplicates

Keeping only useful columns

## 3.1 Cleaning df_crashes
Steps:
1. Fix datetime safely  
2. Handle missing values (decide drop/impute/leave)  
3. Convert numeric columns  
4. Remove impossible/outlier values  
5. Remove duplicates  
6. Create a clean copy (df_crashes_clean)
"""

df_crashes_clean = df_crashes.copy()

df_crashes_clean["CRASH_DATETIME"] = pd.to_datetime(
    df_crashes_clean["CRASH DATE"].astype(str) + " " + df_crashes_clean["CRASH TIME"].astype(str),
    errors="coerce"
)

df_crashes_clean["CRASH_YEAR"] = df_crashes_clean["CRASH_DATETIME"].dt.year
df_crashes_clean["CRASH_MONTH_NUM"] = df_crashes_clean["CRASH_DATETIME"].dt.month
df_crashes_clean["CRASH_DAY"] = df_crashes_clean["CRASH_DATETIME"].dt.day
df_crashes_clean["CRASH_HOUR"] = df_crashes_clean["CRASH_DATETIME"].dt.hour
df_crashes_clean["CRASH_WEEKDAY"] = df_crashes_clean["CRASH_DATETIME"].dt.day_name()

display(df_crashes_clean[["CRASH_DATETIME","CRASH_YEAR","CRASH_HOUR","CRASH_WEEKDAY"]].head())

"""# 3.1.2 – Missing values in key columns

"""

key_cols = [
    "BOROUGH",
    "LATITUDE", "LONGITUDE",
    "ON STREET NAME", "CROSS STREET NAME",
    "NUMBER OF PERSONS INJURED",
    "NUMBER OF PERSONS KILLED"
]

df_crashes_clean[key_cols].isna().sum()

# Fill numeric injury/fatality NaNs with 0
injury_cols = [
    "NUMBER OF PERSONS INJURED",
    "NUMBER OF PERSONS KILLED",
    "NUMBER OF PEDESTRIANS INJURED",
    "NUMBER OF PEDESTRIANS KILLED",
    "NUMBER OF CYCLIST INJURED",
    "NUMBER OF CYCLIST KILLED",
    "NUMBER OF MOTORIST INJURED",
    "NUMBER OF MOTORIST KILLED"
]

for c in injury_cols:
    if c in df_crashes_clean.columns:
        df_crashes_clean[c] = df_crashes_clean[c].fillna(0)

df_crashes_clean["BOROUGH"] = df_crashes_clean["BOROUGH"].fillna("Unknown")

"""# 3.1.3 – Convert numeric fields to numbers safely

"""

for c in injury_cols:
    df_crashes_clean[c] = pd.to_numeric(df_crashes_clean[c], errors="coerce").fillna(0).astype(int)

"""# 3.1.4 – Remove impossible negative values

"""

for c in injury_cols:
    neg_count = (df_crashes_clean[c] < 0).sum()
    if neg_count > 0:
        print(f"Correcting {neg_count} negative values in {c}")
    df_crashes_clean.loc[df_crashes_clean[c] < 0, c] = 0

"""# 3.1.5 – Remove exact duplicates

"""

before = len(df_crashes_clean)
df_crashes_clean = df_crashes_clean.drop_duplicates()
after = len(df_crashes_clean)

print("Duplicates removed:", before - after)

"""#3.1.6 – Finished cleaning crashes"""

print("df_crashes_clean shape:", df_crashes_clean.shape)
df_crashes_clean.head()

"""## 3.2 Cleaning df_persons

Steps:
1. Keep only relevant columns  
2. Handle missing values  
3. Convert numeric fields  
4. Fix person type categories  
5. Remove duplicates  
6. Create df_persons_clean

"""

keep_cols = [
    "COLLISION_ID",
    "PERSON_TYPE",
    "PERSON_INJURY",
    "PERSON_AGE",
    "PERSON_SEX",
    "EJECTION"
]

df_persons_clean = df_persons[keep_cols].copy()
df_persons_clean.head()

"""#3.2.2 – Handle missing values"""

df_persons_clean["PERSON_TYPE"] = df_persons_clean["PERSON_TYPE"].fillna("Unknown")

df_persons_clean["PERSON_INJURY"] = df_persons_clean["PERSON_INJURY"].fillna("Unknown")

df_persons_clean["PERSON_AGE"] = pd.to_numeric(df_persons_clean["PERSON_AGE"], errors="coerce")

"""# 3.2.3 – Standardize categories"""

df_persons_clean["PERSON_TYPE"] = df_persons_clean["PERSON_TYPE"].str.upper().str.strip()
df_persons_clean["PERSON_INJURY"] = df_persons_clean["PERSON_INJURY"].str.upper().str.strip()

df_persons_clean["PERSON_TYPE"].value_counts().head(10)

"""**3.2.4 – Remove impossible values**"""

# Remove impossible negative ages
neg_age = (df_persons_clean["PERSON_AGE"] < 0).sum()
if neg_age > 0:
    print("Correcting negative ages:", neg_age)
df_persons_clean.loc[df_persons_clean["PERSON_AGE"] < 0, "PERSON_AGE"] = np.nan

"""**3.2.5 – Remove duplicates**"""

before = len(df_persons_clean)
df_persons_clean = df_persons_clean.drop_duplicates()
after = len(df_persons_clean)

print("Duplicates removed (persons):", before - after)

df_persons_clean.info()
df_persons_clean.head()

"""# **Phase 4 – Integration (Crashes + Persons)**

Goal:
- Convert df_persons_clean (person-level) into collision-level features
- Join with df_crashes_clean on COLLISION_ID
- Produce a final, unified dataset df_full

**4.1.1 – Prepare safe categories**
"""

df_persons_clean["PERSON_TYPE"] = df_persons_clean["PERSON_TYPE"].str.upper().str.strip()
df_persons_clean["PERSON_INJURY"] = df_persons_clean["PERSON_INJURY"].str.upper().str.strip()

df_persons_clean["PERSON_TYPE"].value_counts().head()

"""**4.1.2 – Aggregate persons into collision-level counts**

This aggregation produces meaningful features for Dash analysis.
"""

person_agg = df_persons_clean.groupby("COLLISION_ID").agg(
    persons_total = ("PERSON_TYPE", "count"),

    # By person type (according to your actual data)
    occupants   = ("PERSON_TYPE", lambda x: (x == "OCCUPANT").sum()),
    pedestrians = ("PERSON_TYPE", lambda x: (x == "PEDESTRIAN").sum()),
    cyclists    = ("PERSON_TYPE", lambda x: (x == "BICYCLIST").sum()),

    # By injury severity
    injured       = ("PERSON_INJURY", lambda x: (x == "INJURED").sum()),
    killed        = ("PERSON_INJURY", lambda x: (x == "KILLED").sum()),
    injury_unknown= ("PERSON_INJURY", lambda x: (x == "UNKNOWN").sum())
).reset_index()

person_agg.head()

"""# **4.2 Join the aggregated persons table into df_crashes_clean**

**4.2.1 – Confirm collisions exist in both tables**
"""

intersect_ids = set(df_crashes_clean["COLLISION_ID"]).intersection(set(person_agg["COLLISION_ID"]))
print("Collisions present in both datasets:", len(intersect_ids))

"""**4.2.2 – Perform the left join**

We want every crash, even crashes with no person data → use a LEFT MERGE
"""

# 4.2.2 – Merge (LEFT join) – rerun this and the fillna block
df_full = df_crashes_clean.merge(person_agg, on="COLLISION_ID", how="left")

cols_to_fill = [
    "persons_total", "occupants", "pedestrians", "cyclists",
    "injured", "killed", "injury_unknown"
]
for c in cols_to_fill:
    if c in df_full.columns:
        df_full[c] = df_full[c].fillna(0).astype(int)

df_full["HAS_PEDESTRIAN"] = (
    df_full["NUMBER OF PEDESTRIANS INJURED"] + df_full["NUMBER OF PEDESTRIANS KILLED"] > 0
)

df_full["HAS_CYCLIST"] = (
    df_full["NUMBER OF CYCLIST INJURED"] + df_full["NUMBER OF CYCLIST KILLED"] > 0
)

df_full["HAS_DRIVER"] = (
    df_full["NUMBER OF MOTORIST INJURED"] + df_full["NUMBER OF MOTORIST KILLED"] > 0
)
for c in ["HAS_PEDESTRIAN","HAS_CYCLIST","HAS_DRIVER"]:
    print(df_full[c].value_counts())

"""# **4.3 Post-join cleaning**

**4.3.1 – Fill NaN for aggregated columns (means 0 persons involved)**
"""

cols_to_fill = [
    "persons_total", "drivers", "passengers", "pedestrians", "cyclists",
    "injured", "killed", "injury_unknown"
]

for c in cols_to_fill:
    if c in df_full.columns:
        df_full[c] = df_full[c].fillna(0).astype(int)

"""**4.3.2 – Final column ordering**"""

ordering = [
    "COLLISION_ID", "CRASH_DATETIME", "CRASH DATE", "CRASH TIME",
    "CRASH_YEAR", "CRASH_MONTH_NUM", "CRASH_DAY", "CRASH_HOUR", "CRASH_WEEKDAY",
    "BOROUGH", "ZIP CODE", "LATITUDE", "LONGITUDE"
]

# Only include columns that exist
ordering = [c for c in ordering if c in df_full.columns]
df_full = df_full[ordering + [c for c in df_full.columns if c not in ordering]]

"""# **4.4 Final check**"""

print("df_full shape:", df_full.shape)
df_full.head()

df_full.isna().sum().sort_values(ascending=False).head(20)

df_full.to_csv("df_full_clean.csv", index=False)
print("Saved df_full_clean.csv")

"""
## **5.1 Check missing values after merging**

We inspect remaining NaNs. After merge, only streets and coordinates are allowed to have NaNs.
"""

missing_full = df_full.isna().sum().sort_values(ascending=False)
missing_full.head(25)

"""# **5.2 Handle coordinates**
This step ensures Dash maps will always work.
"""

# 5.2 – Replace zero coordinates with NaN (if exist)
df_full["LATITUDE"] = df_full["LATITUDE"].replace(0, np.nan)
df_full["LONGITUDE"] = df_full["LONGITUDE"].replace(0, np.nan)

"""#**5.3 Fix data types for final dataset**

We ensure:

Dates → datetime

Numbers → integers or floats

Categories → strings
"""

# Datetime
df_full["CRASH_DATETIME"] = pd.to_datetime(df_full["CRASH_DATETIME"], errors="coerce")

# Numerics (safe conversion)
numeric_cols = [
    "NUMBER OF PERSONS INJURED", "NUMBER OF PERSONS KILLED",
    "persons_total", "drivers", "passengers", "pedestrians", "cyclists",
    "injured", "killed"
]

for c in numeric_cols:
    if c in df_full.columns:
        df_full[c] = pd.to_numeric(df_full[c], errors="coerce").fillna(0).astype(int)

"""# **5.4 Remove duplicate columns accidentally created in merge**"""

dup_cols = [c for c in df_full.columns if c.endswith("_x") or c.endswith("_y")]
if dup_cols:
    print("Removing duplicate columns:", dup_cols)
    df_full = df_full.drop(columns=dup_cols)

"""**5.5 Handle text columns consistently**"""

text_cols = ["BOROUGH", "ON STREET NAME", "CROSS STREET NAME"]

for c in text_cols:
    if c in df_full.columns:
        df_full[c] = df_full[c].fillna("").astype(str).str.upper().str.strip()

"""**Final review**"""

print("Final df_full shape:", df_full.shape)
df_full.info()
df_full.head()

df_full.to_csv("df_final_clean.csv", index=False)
print("Saved → df_final_clean.csv")

"""# **Phase 6 – Feature Engineering + Research Questions**

Goals:
- Add new columns that improve analysis and Dash interactivity.
- Create time-based, severity-based, and road-user–based features.
- Convert df_full into a more insight-rich dataset.
- Define research questions we will answer in Dash.

**6.1 Add Time-Based Features**

Now we add:

Peak hour flag

Day/Night flag

Weekend flag

Season (Winter/Spring/Summer/Fall)
"""

# 6.1 – Time-based engineered features

df_full["IS_WEEKEND"] = df_full["CRASH_WEEKDAY"].isin(["Saturday", "Sunday"])

df_full["DAY_NIGHT"] = df_full["CRASH_HOUR"].apply(
    lambda h: "Night" if (h < 6 or h >= 20) else "Day"
)

df_full["PEAK_HOUR"] = df_full["CRASH_HOUR"].apply(
    lambda h: "Peak" if h in [7,8,9,16,17,18] else "Off-Peak"
)

def get_season(month):
    if month in [12,1,2]: return "Winter"
    if month in [3,4,5]: return "Spring"
    if month in [6,7,8]: return "Summer"
    if month in [9,10,11]: return "Fall"
    return "Unknown"

df_full["SEASON"] = df_full["CRASH_MONTH_NUM"].apply(get_season)

df_full[["CRASH_HOUR","IS_WEEKEND","DAY_NIGHT","PEAK_HOUR","SEASON"]].head()

"""**6.2 Severity Features**
Total injured

Total killed

Severity index = injured + 5*killed

Severity level (Low / Medium / High / Critical)
"""

# 6.2 – Severity engineered features

df_full["TOTAL_INJURED"] = df_full["NUMBER OF PERSONS INJURED"]
df_full["TOTAL_KILLED"]  = df_full["NUMBER OF PERSONS KILLED"]

df_full["SEVERITY_INDEX"] = (
    df_full["TOTAL_INJURED"] + df_full["TOTAL_KILLED"] * 5
)

def severity_level(x):
    if x == 0: return "Low"
    if x <= 3: return "Medium"
    if x <= 10: return "High"
    return "Critical"

df_full["SEVERITY_LEVEL"] = df_full["SEVERITY_INDEX"].apply(severity_level)

"""**6.3 Road User Features (Drivers / Cyclists / Pedestrians)**"""

df_full["HAS_PEDESTRIAN"] = df_full["pedestrians"] > 0
df_full["HAS_CYCLIST"] = df_full["cyclists"] > 0
df_full["HAS_DRIVER"] = df_full["drivers"] > 0

df_full[["pedestrians","cyclists","drivers","HAS_PEDESTRIAN","HAS_CYCLIST","HAS_DRIVER"]].head()

"""**6.4 Speed-Related & Weather Features (Simple Flags)**"""

if "CONTRIBUTING FACTOR VEHICLE 1" in df_full.columns:

    df_full["SPEEDING_FLAG"] = df_full["CONTRIBUTING FACTOR VEHICLE 1"].str.contains(
        "speed|speeding|unsafe speed", case=False, na=False
    )

    df_full["DISTRACTION_FLAG"] = df_full["CONTRIBUTING FACTOR VEHICLE 1"].str.contains(
        "distract|phone|text|inattention", case=False, na=False
    )

    df_full["IMPAIRMENT_FLAG"] = df_full["CONTRIBUTING FACTOR VEHICLE 1"].str.contains(
        "alcohol|drugs|impair|intox", case=False, na=False
    )

"""**6.5 Borough-Level Grouping Features**"""

borough_stats = df_full.groupby("BOROUGH").agg(
    crashes=("COLLISION_ID", "count"),
    injuries=("TOTAL_INJURED", "sum"),
    fatalities=("TOTAL_KILLED", "sum"),
).reset_index()

borough_stats.head()

df_full.info()
df_full.head()
print("Final dataset ready for visualization and Dash.")

df_full.to_csv("df_full_features.csv", index=False)
print("Saved: df_full_features.csv")

"""# **Phase 7 – Visualization Prototyping**

Goal:
- Build all major visualizations that will later be embedded in Dash.
- Ensure each plot works well with df_full_features.csv.

"""

df_full = pd.read_csv("/content/df_full_features.csv")
df_full["CRASH_DATETIME"] = pd.to_datetime(df_full["CRASH_DATETIME"], errors="coerce")

"""#7.1 Bar Charts

**7.1.1 – Crashes by Borough**
"""

df_borough = df_full["BOROUGH"].value_counts().reset_index()
df_borough.columns = ["BOROUGH", "COUNT"]

fig_borough = px.bar(
    df_borough,
    x="BOROUGH",
    y="COUNT",
    labels={"BOROUGH": "Borough", "COUNT": "Number of Crashes"},
    title="Crashes by Borough"
)

fig_borough.show()

"""**7.1.2 – Severity Levels**"""

df_sev = df_full["SEVERITY_LEVEL"].value_counts().reset_index()
df_sev.columns = ["SEVERITY_LEVEL", "COUNT"]

fig_severity = px.bar(
    df_sev,
    x="SEVERITY_LEVEL",
    y="COUNT",
    labels={"SEVERITY_LEVEL": "Severity Level", "COUNT": "Number of Crashes"},
    title="Crash Severity Distribution"
)

fig_severity.show()

"""**7.1.3 – Crashes involving pedestrians / cyclists**"""

df_road = pd.DataFrame({
    "Category": ["Pedestrian", "Cyclist", "Driver"],
    "Count": [
        df_full["HAS_PEDESTRIAN"].sum(),
        df_full["HAS_CYCLIST"].sum(),
        df_full["HAS_DRIVER"].sum()
    ]
})

fig_road_users = px.bar(
    df_road,
    x="Category", y="Count",
    title="Crashes Involving Each Road User Category"
)
fig_road_users.show()

# Do these columns exist?
print(df_full.columns)

# If they exist, how many True values?
for c in ["HAS_PEDESTRIAN", "HAS_CYCLIST", "HAS_DRIVER"]:
    if c in df_full.columns:
        print(c, "value_counts:")
        print(df_full[c].value_counts(dropna=False))
    else:
        print(c, "NOT FOUND in df_full")

df_persons_clean["PERSON_TYPE"].value_counts()

df_persons_clean.head()
df_persons_clean["COLLISION_ID"].nunique(), df_full["COLLISION_ID"].nunique()

df_full["HAS_PEDESTRIAN"] = df_full["NUMBER OF PEDESTRIANS INJURED"] > 0
df_full["HAS_CYCLIST"]    = df_full["NUMBER OF CYCLIST INJURED"] > 0
df_full["HAS_DRIVER"]     = df_full["NUMBER OF MOTORIST INJURED"] > 0

for c in ["HAS_PEDESTRIAN","HAS_CYCLIST","HAS_DRIVER"]:
    print(df_full[c].value_counts())

df_full["HAS_PEDESTRIAN"] = df_full["NUMBER OF PEDESTRIANS INJURED"] > 0
df_full["HAS_CYCLIST"]    = df_full["NUMBER OF CYCLIST INJURED"] > 0
df_full["HAS_DRIVER"]     = df_full["NUMBER OF MOTORIST INJURED"] > 0

for c in ["HAS_PEDESTRIAN","HAS_CYCLIST","HAS_DRIVER"]:
    print(df_full[c].value_counts())

df_persons_clean["PERSON_TYPE"].value_counts()

"""#7.2 Line Charts

**7.2.1 – Crashes per Year**
"""

year_counts = df_full["CRASH_YEAR"].value_counts().sort_index()

fig_year = px.line(
    x=year_counts.index,
    y=year_counts.values,
    markers=True,
    title="Crashes by Year",
    labels={"x": "Year", "y": "Number of Crashes"}
)
fig_year.show()

"""**7.2.2 – Monthly Trend (Across All Years)**"""

df_full["YEAR_MONTH"] = df_full["CRASH_DATETIME"].dt.to_period("M").astype(str)

monthly_counts = df_full["YEAR_MONTH"].value_counts().sort_index()

fig_monthly = px.line(
    x=monthly_counts.index,
    y=monthly_counts.values,
    title="Monthly Crash Trend",
    labels={"x": "Year-Month", "y": "Crashes"}
)
fig_monthly.show()

"""#7.3 Heat maps

**7.3.1 – Hour vs. Weekday Heatmap**
"""

pivot_hw = df_full.pivot_table(
    index="CRASH_WEEKDAY",
    columns="CRASH_HOUR",
    values="COLLISION_ID",
    aggfunc="count"
).fillna(0)

fig_heat = px.imshow(
    pivot_hw.loc[["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]],
    aspect="auto",
    color_continuous_scale="Inferno",
    title="Crash Density: Hour vs. Weekday",
    labels={"color": "Crash Count"}
)
fig_heat.show()

"""**7.3.2 – Borough vs. Severity Heatmap**"""

pivot_bs = df_full.pivot_table(
    index="BOROUGH",
    columns="SEVERITY_LEVEL",
    values="COLLISION_ID",
    aggfunc="count"
).fillna(0)

fig_boro_sev = px.imshow(
    pivot_bs,
    aspect="auto",
    color_continuous_scale="Viridis",
    title="Borough × Severity Heatmap",
    labels={"color": "Crash Count"}
)
fig_boro_sev.show()

"""#7.4 Map Visualizations (Scatter + Density)"""

df_map = df_full.dropna(subset=["LATITUDE","LONGITUDE"])

"""**7.4.1 – Scatter Map (fast prototype)**"""

fig_map_scatter = px.scatter_mapbox(
    df_map.sample(n=min(5000, len(df_map)), random_state=42),
    lat="LATITUDE",
    lon="LONGITUDE",
    color="BOROUGH",
    zoom=9,
    height=600,
    title="Crash Map (Sampled)",
    mapbox_style="carto-positron"
)
fig_map_scatter.show()

"""**7.4.2 – Density Heatmap**"""

pivot_bs = df_full.pivot_table(
    index="BOROUGH",
    columns="SEVERITY_LEVEL",
    values="COLLISION_ID",
    aggfunc="count"
).fillna(0)

import plotly.express as px

fig_boro_sev = px.imshow(
    pivot_bs,
    aspect="auto",
    color_continuous_scale="Viridis",
    title="Borough × Severity Heatmap",
    labels={"color": "Crash Count"}
)
fig_boro_sev.show()

"""#7.5 “Generate Report” Visuals (Dash Summary Cards)"""

total_crashes = len(df_full)
total_injured = df_full["TOTAL_INJURED"].sum()
total_killed = df_full["TOTAL_KILLED"].sum()
top_borough = df_full["BOROUGH"].value_counts().idxmax()

summary = {
    "Total Crashes": total_crashes,
    "Total Injuries": total_injured,
    "Total Fatalities": total_killed,
    "Most Dangerous Borough": top_borough
}

summary